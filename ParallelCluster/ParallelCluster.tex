%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[russian]{article}
\usepackage[T2A,T1]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[a4paper]{geometry}
\geometry{verbose,tmargin=2cm,bmargin=2cm,lmargin=2cm,rmargin=2cm}
\usepackage{algorithm2e}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage[authoryear]{natbib}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\DeclareRobustCommand{\cyrtext}{%
  \fontencoding{T2A}\selectfont\def\encodingdefault{T2A}}
\DeclareRobustCommand{\textcyr}[1]{\leavevmode{\cyrtext #1}}
\AtBeginDocument{\DeclareFontEncoding{T2A}{}{}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
  \theoremstyle{plain}
  \newtheorem*{cor*}{\protect\corollaryname}
  \theoremstyle{definition}
  \newtheorem*{example*}{\protect\examplename}
\newenvironment{lyxcode}
{\par\begin{list}{}{
\setlength{\rightmargin}{\leftmargin}
\setlength{\listparindent}{0pt}% needed for AMS classes
\raggedright
\setlength{\itemsep}{0pt}
\setlength{\parsep}{0pt}
\normalfont\ttfamily}%
 \item[]}
{\end{list}}
\theoremstyle{plain}
\newtheorem{thm}{\protect\theoremname}
  \theoremstyle{definition}
  \newtheorem{example}[thm]{\protect\examplename}
  \theoremstyle{remark}
  \newtheorem*{notation*}{\protect\notationname}
  \theoremstyle{plain}
  \newtheorem*{lyxalgorithm*}{\protect\algorithmname}
  \theoremstyle{plain}
  \newtheorem*{thm*}{\protect\theoremname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{citehack}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{indentfirst}
\usepackage{cmap}
\usepackage{tikz}

\makeatother

\usepackage{babel}
  \providecommand{\algorithmname}{Алгоритм}
  \providecommand{\corollaryname}{Вывод}
  \providecommand{\examplename}{Пример}
  \providecommand{\notationname}{Нотация}
  \providecommand{\theoremname}{Теорема}
\providecommand{\theoremname}{Теорема}

\begin{document}

\title{Параллельные и распределенные вычисления}


\author{Akhtyamov Pavel}

\maketitle

\part{Параллельные вычисления}


\section{Характеристики параллельных программ}

Закон Амдала строится для ускорения $S(p)$. Есть части кода последовательные
и параллельные. Положим $1=\alpha+\beta$, где $\alpha$ --- доля
последовательной части кода, а $\beta$ --- параллельной. Как тогда
можно посчитать ускорение: ранее была обозначена следующая формула
для ускорения: $S(n)=\frac{T(1)}{T(n)}=\frac{1}{\alpha+\frac{\beta}{n}}=\frac{1}{\alpha+\frac{1-\alpha}{n}}\ (1)$.
Тогда какое максимальное ускорение можно получить в принципе? Действительно,
можно определить $\underset{n\rightarrow\infty}{\lim}\frac{1}{\alpha+\frac{1-\alpha}{n}}=\frac{1}{\alpha}$.
К примеру, если $\alpha=\frac{1}{2}$, то программу нельзя ускорить
более, чем в два раза.
\begin{cor*}
Для того, чтобы получить ускорение, необходимо уменьшить временную
долю выполнения последовательной части.
\end{cor*}
На самом деле, описанная выше модель является идеальной. На самом
деле, в реальных системах существуют накладные расходы. Поэтому с
учетом накладных расходов, формула $(1)$ переписывается в следующем
виде: $S(n)=\frac{1}{\alpha+\rho(n)+\frac{\beta}{n}}$, причем параметр
зависит от числа процессоров, входных данных.

Накладные расходы уходят на:
\begin{enumerate}
\item Порождение ``объектов'' (не зависит от объёма входных данных).
\item Синхронизация (не зависит от объёма входных данных, явно не очевидна,
ибо зачастую выполняется засчёт пересылки данных) --- плохо при MPI\_Barrier
на 10000 процессах.
\item Барьерные эффекты (проблема вместимости в кеш, сверхлинейное ускорение,
к примерам) --- зависят от аппаратуры (есть в последовательном эффекте,
но они явно не выражаются, ибо выполняется код только на одной машине).
\item Накладные расходы на передачу данных (наиболее сильно проявляется
при передачи данных по сети --- Interconnect, менее --- при передачи
одного процессора к другому). К примеру, есть $t_{start}+dataSize\cdot(maxflow)$,
где $maxflow$ --- максимальная пропускная способность.
\item Накладные расходы на вход/выход.\end{enumerate}
\begin{example*}
Предположим, что у нас есть некий массив длины $n$, имеем процессоров
в количестве $P$ штук. Мы хотим посчитать сумму чисел на массиве.
Положим за единицу времени --- время вычисления суммы, а за двойку
--- время передачи по сети четырех байт информации. Посчитаем время,
затраченное на подсчет суммы.
\end{example*}
Далее автор пошел считать :-)
\begin{cor*}
При передаче данных нет выигрыша вообще, если нет --- то ускорения
на 3 процессорах будет достаточно, и, вообще, оптимальное ускорение
в идеальной модели при $n\rightarrow\infty$ получается при количестве
процессоров, равных $p\sim\sqrt{n}$.
\end{cor*}
Далее рассмотрим вопрос о зависимостях и связях между ними. У нас
есть код, в нем есть операторы и переменные (переменные могут меняться
операторами). Обозначим через $s_{i}$ --- $i$-ый оператор языка.
\begin{lyxcode}
int~a,b;

a~=~

1;

b~=~10;
\end{lyxcode}
Мы смотрим ход выполнения программы (то, что написано позже, может
выполняться раньше, к примеру, генерация конструкторов). А можем ли
мы поменять процесс выполнения операций для получения ускорения работы
программы? Обозначим $In$ --- множество переменных, которые необходимы
для данного оператора, а $Out$ --- множество переменных, которое
меняется после выполнения операции (обычно, некоторого набора операции,
но надо уточнять). Говорят, что операторы находятся в зависимости,
если они имеют непересекающийся набор необходимых операторов.

Заметим, что зависимоть $In$-$In$ не накладывает никаких ограничений.
Зависимость $Out\rightarrow In$ является истинной: данные операторы
нельзя поменять местами. Следующая зависимость $In\rightarrow Out$,
является очень странной. Её иллюстрирует следующий пример:
\begin{lyxcode}
b~=~1;

a~=~b~+~1;

c~=~b~+~2;

b~=~$\emptyset$;
\end{lyxcode}
Говорят, что переменная $b$ находится в антизависимости. И при хорошем
надлежании, от нее можно избавиться, но надо следить глобально за
кодом (замечание с лекции), ибо в данном примере после кода нам неважно
значение переменной $b$.

Зависимость $Out\rightarrow Out$ плоха в том случае, если мы не знаем,
что происходило между этими операциями.

Всеми зависимостями занимается компилятором. Можно построить граф
зависимостей по данным для операторов, на вершинах записать время
срабатывания оператора ($\tau_{i}$). Тогда время выполнения графа
--- длина наибольшей цепочки (критический путь). Хорошо использовать
количество процессоров, равное ширине максимального уровня. Если есть
транзитный путь и прямой путь в таком графе, то лучше удалить прямой
путь. В суперскалярной архитектуре можно решить, куда направить вычисление
для вентилей. 

Какие есть проблемы:
\begin{enumerate}
\item Наличие указателей --- мы не можем сказать, что происходит с памятью
под этими указателями. Но компилятор может найти набор заклинаний
и распознать их. Иногда можно распознать параллелизм по наличию некоторых
шаблонов для получения информации.
\item Наличие вызовов разных функций --- о внутреннем строении сложно что-либо
сказать.
\end{enumerate}

\section{Суперкомпуктеры}

Поймем, как измеряется производительность суперкомпьютеров. FLOPS
(Floating Operations Per Second) - количество операций с плавающей
точкой, которое можно делать в секунду.
\begin{example}
Персональный компьютер - 6 ядер, 16 Gb оперативки, 6Tb HDD, 60 GFLOPS,
Windows, Linux, Mac OS - что можно запихать в один узел.
\end{example}
Текущая производительность - между пента и экзофлопсом. Компьютер
экзофлопсной производительности будет построен на других основах,
считают специалисты (надо будет, к примеру, переделать OS).
\begin{example}
БЭСМ-6 (1968) - Большая Электронно-Счётная Машина. Производительность
- 1 MIPS ($10^{6}$ операций - инструкций в секунду, плавающей точки
не было --- эмуляция на целочисленной арифметике), 1 процессоров,
OS - КРАБ, RAM 128 Kb, объем - 512 Kb на барабане, 3 Mb на ленте.
\end{example}

\begin{example}
``Ломоносов'', 2010 год. Производительность --- 397 TFLOPS, число
процессоров --- 10260 (ядер 41040), OS --- Clustrx (Linux), RAM 73920Gb,
места --- 1382400Gb (80 место в мире) --- занимает приблизительно
лекционную аудиторию (есть Ломоносов версии 2 --- занимает первое
место в стране, 31 место в списке).
\end{example}

\begin{example}
К, Япония, 2011 год (5 место). Производительность --- 10510 TFLOPS,
число процессоров --- 88128 (ядер 705024), OS - mod Linux, RAM 1410048Gb
--- занимает место ангара. Построен достаточно большой центр --- большое
здание охлаждает компьютер (жидкостное охлаждение, крутые процессоры
--- обогревание районов, энергии жрет как один микрорайон --- 12 MВатт)
--- на первом месте в Green 500.
\end{example}


Крутость определяется тестом по перемножению матриц, и количество
операций в секунду --- характеристика машины. Есть два списка - Top-500
и Green-500 -- производительность делится на потребленную мощность.
Сегодня, считают, что более важные операции --- действия на графах.
Поэтому появился список Top-Graph-500 (на первом месте - все тот же
японский компьютер). Но в них есть проблема --- много хаотических
передач данных, т.е. он лучше работает по передаче. Тест построен
так: обход графа в ширину, при этом мера производительности --- количество
тысяч ребер, обрабатываемых за одну секунду. Итак, данный тест проверяет
случаи, в которых не так много вычислений, как передачи данных.

Тенденции в мире по поводу OS: в целом, все работают под Linux. По
организациям --- по количеству используется в Research области (прикладной,
проверка правильности технологических решений), Science (Academic)
занимается устройством мира в целом, по количеству --- индустрия (проектирование
всякой всячины). В нашей стране --- по науке и образованию в 2011
году (сейчас уже доля промышленности выросла по этой мере). По графику
видно, что в один момент ударились в образовании.

Есть специальные тесты для проверки коммуникаций, по которым можно
проверять скорость (чем темнее, тем медленнее). В ``Ломоносове''
шум неравномерно распределен по системе (причем общая память намного
шустрее работает, чем передача между узлами).

Как можно измерять производительность суперкомпьютера:
\begin{enumerate}
\item Просто посчитать (IPS) --- более-менее для целочисленной арифметики.

\begin{enumerate}
\item Пиковая производительность --- максимальная теоретическая производительность
(не достигается).
\item Тесты от производителя --- оптимизация железа (если будет использоваться
определенный тест, то разработчики подгоняют железо под параметры
теста). Top-500 (HPL написаны на C (Джек Донгара) --- используется
матрица и методом Гаусса решается система линейных уравнений, есть
проблемы с выбором строки или столбца; разработчик может подогнать
размер матрицы -- подходило для кеша, к примеру) подвергается большой
критике.
\item Реальная производительность на задачах
\end{enumerate}
\item Система тестов NASA Parallel Banchmark, Fortran (люди решили написать
более интеллектуальные тесты), использующая разные модели распараллеливания,
используются практические задачи (к примеру, запуск и полет космических
кораблей). Есть 13 областей для распараллеливания.

\begin{enumerate}
\item MG (MultiGrid) --- решается система дифференциальных уравнений (теплопроводности,
к примеру), и есть много различных сеток --- проблема совмещения;
\item CG (Conjugate Gradients);
\item FT - метод быстрого преобразования Фурье (FFTW) --- возникает хаотический
паттерн (Butterfly) для обращения в память;
\item IS (Integer Sort) --- берется набор чисел, разбитый по определенным
блокам, в меру интенсивный обмен;
\item EP --- генерация независимых случайных величин, конкретная оценка
работы узлов в кластере по отдельности;
\item SP;
\item LU --- аналог HPL;
\item UA --- уравнение теплопроводности в кубе при мигрирующем источнике
тепла (там, где тепла много, надо сгущать сетку --- проблемы при обращении
в память);
\end{enumerate}
\item HPCG (Джек Донгара, написан на C++) --- система линейных уравнений
методом сопряженных градиентов (минимум на квадратичной функции),
алгоритм зашит + типы матриц, но можно использовать свои данные (под
каждый тип --- определенный способ передачи информации через сеть),
поэтому всякая неэффективность вылезет сразу. Матрицы берутся из разных
прикладных областей. Есть серьезные отличия от Graph-500: здесь есть
серьезный обмен и вычисления.
\end{enumerate}

\section{Infiniband}

Infiniband --- самая популярная система интерконнекта, которая на
сегодняшний день применяется в кластерах. Одним из важнейших отличий
Infiniband --- ограничения на провода: (1) Infiniband через оптоволокно;
(2) Infiniband через медь.

Бывает Qdr (количество проводов внутри кабеля --- влияет на стоимость
технологии: Infiniband гарантирует, что коллизии между проводами не
будет). Технология Infiniband поддерживает RMA (Remote Memory Access)
--- инициированный пакет находится не только в сетевухе, но и может
оказаться сразу на месте (позволяет экономить на время доставки).
Далее, в Infiniband есть широковещание --- широковещательный пакет
внутрь Infiniband (а не внутрь Ethernet, как обычно понимается).

В Infiniband есть три понятия:
\begin{enumerate}
\item HCA --- host - подключение, причем подключенная к PCI-Express; причем
сейчас выгоднее послать данные, чем записать на свой жесткий диск
(своеобразный кеш).
\item DCA --- host для назначения данных (приемника, отправляется через
switched fabric) --- switch-и соединяются между собой. Более того,
используется для организации хранилищ (storage). Но разделение узлов
через switch есть проблема: чтение из файла мешает для вычисления
данных (напряжение switch). Поэтому делают отдельную сеть для организации
ввода/вывода (обе сети: основная и ввода -- реализованы через Infiniband).
\item Каждое устройство, которое подключено к Infiniband, получает свой
GID (Group Identificator), почти как MAC-адрес, но не совсем:
\item LID (local identifier), и при этом есть subnet manager (знает состояние
Infiniband сети, в случае проблем --- lifetime перестройка данных)
--- id в терминах subnet manager: (если происходит подключение карты
к Internet, то ей присваивается LID). Протокол при начале работы выбирает
несколько фишек --- и если мы не можем их передать, и протокол маршрутизации
выберет новый путь с большим количеством фишек (реализовано на основе
модели сети Петри, см. далее). Задача subnet manager --- выставить
очередной линк и установить величину порога (завуалированная пропускная
способность линка). В Internet (Ethernet) очень сложно работать на
такой модели, ибо внешняя среда непредсказуема. Такой способ организации
сети гарантирует отсутствие коллизий. Пакет не отпускается, пока он
не будет передан (в отличие от Ethernet). При добавлении устройства
прописываются данные в таблицу маршрутизаций (subnet manager делает).
Хорошо для внутренней работы, но не для организации сетей (ибо сеть
будет иметь огромную таблицу маршрутизации, что плохо).
\end{enumerate}
Вопрос: как передать данные? У каждого switch есть набор адресов,
куда надо пересылать данные. (Здесь будет картинка)

\tikz{
	\draw (0,0) -- ++(2,0) -- ++(0,2) -- ++(-2, 0) -- ++(0, -2);
	\draw (2, 1) -- ++(1,0) -- ++(0,-1) -- ++(2, 0) -- ++(0,2) -- ++(-2, 0) -- ++(0, -2);
}
\begin{notation*}
Сеть Петри: есть некоторые узлы (граф) и некоторые пороги. В вершинах
графа существуют фишки --- могут перемещаться из узла в узел. При
этом фишки перескакивают через порог, если в ней накопилось достаточное
число фишек.
\end{notation*}

\section{Обзор технологий параллельного программирования}

Технологии можно построить по уровням. Можно построить пирамиду по
принципу удаленности от железа:
\begin{enumerate}
\item Предоставление ОС (pthread, socket, fork, clone) --- набор системных
вызовов (уровень ОС). Задач: обеспечить переносимое относительно конкретного
железа параллельное использование ресурсов.
\item Низкоуровневые библиотеки параллельного программирования (может использовать
библиотеки из первого уровня, MPI, openMP, CUDA, openshmem). Задача:
абстрагирование от конкретной технологии, через которую происходит
передача данных (к примеру, может использоваться Infiniband или разделяемая
память IPC). Библиотека на Runtime может определить, какую технологию
лучше использовать при передаче данных в рамках узлах кластера. При
вводе/выводе MPI может понять, какую коммуникационную сеть использовать
(а при этом сами не знать, что в действительности произошло). MPI
обеспечивает механизм переносимости ОС. OpenMP предоставляет независимый
интерфейс (но компилятор должен уметь организовывать, но реализация
идет лучшим способом, как в Windows, так и в Linux, и в Android: если
есть thread, то можно использовать определенную опциональность). CUDA
должен обеспечить передачу данных на графическую карту и обратно.
Openshmem будет хорошо работать при наличии RMA (см. выше), иначе
использует MPI.
\item Обертки над вторым уровнем. 

\begin{enumerate}
\item К примеру, binding Python MPI. 
\item Другой способ --- написание высокоуровневых библиотек (на низких уровнях
использует (2), а предоставляет программный интерфейс) --- 

\begin{enumerate}
\item Cublas: (blas -- операции с линейной алгеброй). Для IBM есть реализация
интерфейса BLAS: ESSL, PESSL (использует конкретные инструкции для
IBM), для Intel --- mkl, для графической карты --- cublas. 
\item Curandom --- библиотека для случайных чисел на графической карте (цель:
использование случайных чисел на графической карте). 
\item FFTW --- библиотека для быстрого преобразования Фурье в MPI. 
\item Hypre работает средствами MPI и openMP (при этом в начале происходит
настройка на архитектуру и специфичные тесты: размеры кеша, количество
кешей ит.д.), и при этом перераспределяет данные для вычисления данных
по линейной алгебре.
\item Plasma работает на приципе data-flow: если есть маленькие элементы,
то удобно сразу собирать данные поточно.
\item Petsi --- библиотека линейной алгебры.
\item Cilk++
\end{enumerate}
\item Написание высокоуровневых языков параллельного программирования: компиляция
происходит в две стадии -- (1) Компиляция в код, к примеру, MPI, C++;
(2) натравка компиляторов для параллельных программ. Примеры: DVM
--- ``расширенный MPI + Fortran'', подстановка pragma. Charm++ --
набор charm-ов, которые обмениваются активными сообщениями (вместе
с сообщением приходит код для обработки сообщения). При такой модели
можно избегать глобальных синхронизаций. При этом сообщения обрабатываются
в отложенном режиме (аналогично функциональным языкам): при приходе
сообщения вызывается метод char-а (char -- объект C++, заранее нельзя
сказать, на каком числе будет выполняться код {[}на практике -- не
меняется число узлов{]}).
\item PGAS: X10, UPS. Модель: $n$ процессоров, у каждого своя память, есть
окно, которое будет синхронизироваться для всех процессов (иллюзия
общей памяти --- при выполнении команды put данные будут положены).
Хорошо работает на архитектуре NUMA, (на кластерной архитектуре приемлемо
будет работать только с RMA).
\end{enumerate}
\item Библиотеки и языки предметной области: обычно внутри себя используют
второй уровень, но могут пользоваться третим. Программист даже не
подозревает, что его программа будет работать параллельно (предоставляется
язык, понятный для его предметной области): речь идет о DSL (Domain
Specific Language): все, в итоге, отображается в параллельную программу,
но таких языков все-таки мало. \textbf{Все детали скрыты от программиста! }

\begin{enumerate}
\item Норма: описываются операции над сетками --- необходимо для решения
сеточных уравнений (теплопроводности, колебаний, Навье-Стокса $\rightarrow$
отображается в параллельный код). У человека практически нет возможности
влезать в код (сравнимо с влезанием в ассемблерный код на C++).
\item Games, Gromacs --- движение атомов.
\item Namd --- использует Charm++.
\item Flulend, Open Focus, Flow Vision и огромное количество.
\end{enumerate}
\end{enumerate}

\section{MPI Advanced. Односторонние коммуникации, MPI\_IO}

Некоторые устройства подддерживают RMA. Односторонние коммуникации
сделаны в MPI для их поддержки. Для этого существует механизм окон.
В памяти каждого процесса выделяется область (окно), которая может
располагаться произвольным образом в каждом из процессов (даже может
быть разного размера). При этом, в каждом окне есть локальная составляющая
окна (мы можем знать, где мы находимся в другом процессе --- смещение
относительно начала окна). Параметры узнаются от функций, которые
регистрируют окно. 

\tikz {
	\draw (0,0) -- ++(2,0) -- ++(0,2) -- ++(-2, 0) -- ++(0,-2);
	\draw (0, 0.5) -- ++(2, 0) -- ++(0, 0.5) -- ++(-2, 0) -- ++(0, -0.5);
	\draw (4, 1) -- ++(2, 0) -- ++(0, 0.5) -- ++(-2, 0) -- ++(0, -0.5);
	\draw (4, 0) -- ++(2, 0) -- ++(0,2) -- ++(-2, 0) -- ++(0, -2);
}

При регистрации окна в памяти соответствующего процесса происходит
отображение в сетевом оборудовании на данный процесс (сетевая карта
через PCI-Express кладет данные в память, и процессор в этом мероприятии
не задействован, только через коммуникационное оборудование). Хотим
получить синхронизированные данные в каждом окне. При этом не организовывается
запись в ячейку памяти. Поэтому создается все при помощи функций:
\begin{lyxcode}
MPI\_WIN~win;

MPI\_WIN\_CREATE(...);~//~создаёт~окно

MPI\_Put(void{*}~buffer,~size,~MPI\_Type,~where,);~//~можно~создать~shared~окно~или~private~окно

-{}-{}-~при~этом~тот,~кто~получает~инфу,~не~знает~об~этом,~на~PCI-Express~объявляется~

событие~регистрации~записи

MPI\_Get(...);~//~можно~подсматривать~чужую~память

MPI\_PutR(...);~//~начиная~с~версии~3,~тогда~можно~просматривать~все~через~request-ы

MPI\_GetR(...);~//~начиная~с~версии~3;\end{lyxcode}
\begin{notation*}
Есть схожая библиотека Open Shmem.
\end{notation*}
При этом за чтением и записью одновременной никто не следит. Чтобы
следить за данными, можно использовать следующие функции:
\begin{lyxcode}
MPI\_BARRIER(...);~//~нежелательно

MPI\_WIN\_Fence(...);~//~вводится~состояние~эпохи:~операции~с~локальной~памятью~закончены;~

гарантирует,~что~закончилась~эпоха~

(после~действия~никто~не~производит,~к~примеру,~Put-ы~закончились)

MPI\_Accumulate(...)~//~аналог~Reduce,~выполняет~при~заполнении~необходимые~операции~(используя~RMA)
\end{lyxcode}
В MPI 3 появились коллективные неблокирующие операции:
\begin{lyxcode}
MPI\_IBarrier()

MPI\_IBCast()
\end{lyxcode}
По идее, MPIch может поддерживать такие операциию. Эти штуки крайне
сложно реализовать, поэтому ее не было в стандартах (особенно в аппаратуре).
При помощи новых штук MPI быстрее работает.

Следующая интересная вещь --- MPI\_IO. К примеру, у нас есть файл.
Позволим в этот файл писать разным процессам через коммуникационную
сеть: реализация MPI пытается это оптимизировать (надо ведь доставить
информацию до узлов ввода-вывода). Стандартные интерфейсы могут работать,
но MPI может быстрее. 

В файле создается View. Операция записи производит в View атомарно.
Каждому процессу отдается пространство во View. Каждый из процессов
пишет в свою область файла (тем самым гарантируется, что разные процессы
пишут в разное место, и не надо будет скакать, ибо участки находятся
рядом). Операция происходит посредством многих view. Далее --- устройства
окон.

\tikz {	
	\draw (0,-0.5) -- ++(15, 0) -- ++(0,0.5) -- ++(-15, 0) -- ++(0, -0.5);
	\draw (0,-0.5) -- ++(0.5, 0) -- ++(0, 0.5) -- ++(-0.5, 0) -- ++(0, -0.5);
	\draw (0.5,-0.5) -- ++(0.5, 0) -- ++(0, 0.5) -- ++(-0.5, 0) -- ++(0, -0.5);
	\draw (1,-0.5) -- ++(0.5, 0) -- ++(0, 0.5) -- ++(-0.5, 0) -- ++(0, -0.5);
	\draw (1.5,-0.5) -- ++(0.5, 0) -- ++(0, 0.5) -- ++(-0.5, 0) -- ++(0, -0.5);
	\draw (2,-0.5) -- ++(0.5, 0) -- ++(0, 0.5) -- ++(-0.5, 0) -- ++(0, -0.5);
	\draw (2.5,-0.5) -- ++(0.5, 0) -- ++(0, 0.5) -- ++(-0.5, 0) -- ++(0, -0.5);
}

Функции:
\begin{lyxcode}
MPI\_File\_open();//~открывается~файл

MPI\_Set\_View();~//~создается~окно~-~привязывается~к~типу~данных

MPI\_Read();~//~оптимизирована,~отправляются~в~сумме~(но~возможно~даже,~через~другую~сеть);

MPI\_Read\_at();~//~lseek~+~read

MPI\_Write();~//~

MPI\_Write\_at();~//~lseek~+~write
\end{lyxcode}
Можно профилировать приложения при помощи pmpi, к примеру, исходник
--- P\_MPI\_Send (внутри MPI\_Send вызывается P\_MPI\_Send --- можно
отправлять всю инфу в журнал). Управляется дополнительными ключами
в mpicc.

Не стоит пользоваться client-socket соединениями (один открыл порт
80, организуется сеть через внешний коммуникатор, есть в стандарте:
кому-то надо было или хороший contibutor якобы, пример, слежение данных
за птицами через wi-fi).

Порождение MPI процессов по ходу дела --- MPI\_Comm\_spawn().

В ранних версиях MPI если один процесс погибал, то убивается все приложения.
Сейчас есть MPI\_Err\_Handler --- callback на неблагоприятное событие.
Если кто-то сдох, то устанавливается статус и срабатывает MPI\_Err\_Handler().



\newpage


\part{Распределенные вычисления (Дмитрий Прокопцев, dprokoptsev@gmail.com)}


\section{Основы распределенных систем}

Лучше расширять сеть географически. Проблема состоит в том, что с
ростом узлов вычислительной системы надежность системы будет падать.
Считается, что надежность узла равна $99,5\%$: если у нас 1000 машин,
то в любой момент времени в среднем будет лежать 5 (может рубануть
электричество, сгореть, затопить и т.д.). Никакие подобные события
не будут являться основанием неработоспособности системы (потребности
пользователей необходимо удовлетворять). В целом, мы будем заниматься
построением систем с учетом выходом из строя.

Определений распределенных систем много, но будем считать, что это
та система, которая сохраняет работоспособность при отказе части узлов.

Исторически хотели завуалировать принцип работы системы: надо создать
иллюзии работы на одном узле. К примеру, есть программа, мы хотели,
чтобы отдельные части (функции) работали в разных частях.

К примеру, локальная программа имеет вид:
\begin{lyxcode}
int~divisor(int~x);
\end{lyxcode}
У нас еще маленькая машина, которая вызывает divisor, и огромная машина.
Поэтому если есть клиентский код, затем происходит его упаковка (client
stub), которая декодируется в байт-код, к примеру, JSON, чтобы можно
было разобрать код далее, пересылает RPC-server, пересылает по сети,
распаковывает через server stub, и доходит до server code (и сервер
не знает, где вообще находится код).

Но есть некоторые проблемы: 1) каким образом надо представлять данные
(проблема сериализации); 2) надо написать stub-ы для распаковки и
упаковки (будем описывать интерфейсы на специальном языке: какие функции,
аргументы, как можно сериализовать). Но не всякий язык годен для этого.
К примеру, рассмотрим код на C++
\begin{lyxcode}
void~min\_max(int{*}~begin,~int{*}~end,~int{*}~min,~int{*}~max).
\end{lyxcode}
Но указатели --- это области памяти, поэтому надо будет более высокая
оберка:
\begin{lyxcode}
void~min\_max(int{[}{]}~range,~out~int,~out~int);
\end{lyxcode}
На RPC-сервере остается задание посылки/пересылки/отправки на распаковку.
Поэтому возможна настройка stub-ов на нескольких языках. В некоторых
языках есть свой средства сериализации (Java, Python) --- посылать
напрямую. Но при этом дихотомии на сервере и клиенте не возникает
(один и другой узел может быть использован в качестве сервере).

Но что, если мы хотим не только вызывать функции? Поэтому уже есть
желание получит распределенно-объектные системы (посылать сообщения
от объектов).

Пример банковской системы:
\begin{lyxcode}
void~deposit(acc\_id,~amount);

void~withdraw(acc\_id,~amount);
\end{lyxcode}
Хочется дергать системы, но есть проблема: выполнилась одна из них,
но появилась проблема выполнения другой. Возникает идея транзакций:
но где их вызывать. Непараллельный код транзакций:
\begin{lyxcode}
class~TX~\{
\begin{lyxcode}
void~deposit(...);

void~withdraw(...);

void~commit(...);

void~rollback(...);
\end{lyxcode}
\}

TX{*}~begin\_tx();
\end{lyxcode}
Можно ли перенести это на RPC? Проблема в указателе. Можно переслать
машинный адрес, но нам нужен физический доступ из необходимого места.
К примеру, писать распределенную файловую систему:
\begin{lyxcode}
class~File~\{
\begin{lyxcode}
void~read(void~{*},~size\_t);

void~write(const~void~{*},~size\_t);

void~close();
\end{lyxcode}
\}

File{*}~open(p{*}~dh);
\end{lyxcode}
Но хотим пересылать ссылки на объект. Обычно указывается еще один
объект ссылки, и будем хранить кроме локальных ссылок, еще будут храниться
глобальные ссылки. Поэтому рядом с RPC-серверами в модели появляются
хранилища объектов. И теперь вызов функции будет происходить следующим
образом: как мы будем отправлять указатели: идем в хранилище, достаем
идентификатор, и отправить обратно ($map<ObjID,void*>$). Далее оборачиваем
идентификатор в клиенте, и отдаем обратно. Поэтому у нас будет целый
класс-заглушка (хотим вернуть похожее на прокси-класс). Здесь опять
же нет никакой дихотомии (можно пересылать на разные узлы).

На удаленном вызове процедур была еще проблема: как узнать, где код
должен выполняться. Обычно знания конфигурируются извне (к примеру,
на старте). Но, к примеру, можно присобачить к objId host и port,
получится тройка, полностью идентифицирующая его. Поэтому мы получаем
хороший прокси.

\tikz {	
	\draw (0,0) -- ++(0,2) -- ++(2,0) -- ++(0,-2) -- ++(-2, 0);
	\draw (5,0) -- ++(0,2) -- ++(2,0) -- ++(0,-2) -- ++(-2, 0);
	\draw (2,0.5) -- ++(3,0);
	\draw (2,1.5) -- ++(3,0);
	\draw (7,1) -- ++(3,1);
	\draw (10, 1.5) -- ++(0,1) -- ++(1,0) -- ++(0,-1) -- ++(-1, 0);
	\draw (10, 0) -- ++(0,1) -- ++(1,0) -- ++(0,-1) -- ++(-1, 0);
	\draw (10, -1.5) -- ++(0,1) -- ++(1,0) -- ++(0,-1) -- ++(-1, 0);
}

К примеру, у нас есть узлы 0-99, 100-199, .... Тогда если клиент запрашивает
begin: то обратно пересылается через прокси c $objectID$, и затем
идет общение напрямую, минуя прокси. Но вопрос: некоторые ссылки нужны
уже не будут. Первый подход --- вызов delete: происходит удаление
объекта при вызове, но надо всем договориться, чтобы все корректно
освободили объект и закончили работу с ним. Второй подход --- хранить
число ссылок на объект (и когда никто не используют --- удалять их).
Надо следить за копированием ссылок, следить за циклическими ссылками.
Оба подхода имеют минусы. На локальной машине можно сделать сборку
мусора, но такой объект не подходит. Поэтому проблема освобождения
остается в силе.

Но возникают замечательные проблемы с RPC: если что-то отвалилось/вырубилось
(умерший компьютер). В течение которого времени стоит ожидать ответ?
Мы не знаем политики определения настроек, и как восстанавливать данные.
Но если говорить про TSP, то можно запросить снова. Но есть критичные
для повторения побочные эффекты (все, кроме вычисления результатов
функций).

Далее, предположим, что машина умерла (память сдохла), и все ссылки
умерли. Но хранить ссылки везде плохо --- получается лишний траффик
(пропускной способности не останется). Но еще сложно отличить умершую
машину от умершей сети (выдернули link, а затем вернули, и начали
использовать ссылки их невалидно). Важно помнить, что запросы доходят
только через некоторое время!

Все вышеперечисленные проблемы наделены проблемы излишней абстракции.
И когда мы не знаем семантики, мы не в состоянии обеспечить абстрагировать
распределенность системы, т.е. создавать иллюзию локальности. Решить
проблемы в рамках данной парадигмы практически невозможно.

Все это было в районе девяностых систем. А что, если не пытаться решить
все проблемы сразу? Распределенные вычислительные системы, к примеру,
решают свои проблемы. Распределенные базы данных решают другие проблемы
(хранения данных). Таким образом, если решать конкретные задачи, то
мы получим лучшие результаты, решить больше проблем. В итоге можно
предоставить более хорошую иллюзию.

Сейчас этот метод (без специализации) не очень часто используется,
но в зачаточном состоянии есть наличие RPC. К примеру, в распределенной
базе данных мы хотим читать 10 строк, а затем ``позвонить'' и читать
из курсора еще 10 строк (по сути ссылку на объект БД). Похожие на
RPC системы можно встретить в public interface. (К примеру, Google
Maps API: но + под капотом идут перезапросы, распределителей, обход
проблем внешними средствами).


\section{Парадигмы проблем, виды отказов}

У нас есть распределенная система и часы, которые следят за временем.
Но иногда ломаться может все. Рассмотрим некоторые парадигмы:
\begin{enumerate}
\item Fail-stop --- узел сдох окончательно, требуется реконфигурация.
\item Fail-recovery --- узел помер на время, он может быть восстановлен
самостоятельно или посредством человека. При этом реконфигурация не
требуется. Но есть проблема со стиранием некоторых состояний в узле
(и надо решать актуальность системы)
\end{enumerate}
Заметим, что в этих случаях узлы играют так, как мы ожидаем (посылают
сообщения по протоколу, может быть, ломаются). Но могут быть злонамеренные
узлы (не играющие по правилам):

3. Byzantine (Византийские узлы) --- он может сказать одному узлу,
что живой, а другому --- что умер (или передавать специально неактуальную
среду). Такие системы могут быть созданы, к примеру, для посылки мусора,
или получать секретные данные, поддельные DNS-серверы. Чтобы решать
проблемы, можно собрать общее мнение у узлов (хотя бы $\frac{2}{3}$
от всего количества).

Еще может падать линк (соединение между узлами).
\begin{enumerate}
\item Perfect --- совершенные каналы связи (принимает все сообщения ровно
один раз в том же порядке). Можно воссоздать, к примеру, в switch-е
или доверительной системе.
\item Если сообщения теряются $+-$ честно (Fair-Loss links) --- для любого
сообщения существует положительная вероятность доставки сообщения.
Из Fair-loss можно сделать Perfect (посылать много раз одно сообщение).
\item Byzantine --- перестановка сообщений местами, замена сообщений. Встречается
в более частой среде, нежели чем между узлами (к примеру, Московское
метро). Но при этом, если поставить определенные требования к сообщениям
(к примеру, криптографическое шифрование). Поэтому можно изменить
Byzantine в Fair-Loss.
\end{enumerate}
Таким образом, по транзитивности можно получить из Byzantine соединения
в Perfect. При этом еще можно добиваться подтверждения сообщения.
При этом, если не хотим хранить сообщения, то можно хранить их номер.
Поэтому можно добиться результата с определенной вероятностью успеха.

Но, конечно, не всегда можно различить отрыв линка от отрыва узлов.

Осталось рассмотреть часы. Но мы хотим узнавать факты относительно
производительности системы:
\begin{enumerate}
\item Sync --- есть верхнее ограничение на время пересылки и время обработки
(тогда можно делать предположения относительно нашей системы). К примеру,
если есть протокол, который через время $\tau$ должен выполнять операцию,
то если через $3\tau$ должны получить ответ.
\item Eventually sync --- система почти всегда асинхронна, но есть гарантия,
что наступит определенный момент времени, когда наступит синхронизация.
\item Async --- асинхронные системы.
\end{enumerate}
При написании распределенной системы надо выбрать мир, в котором мы
собираемся создавать распределенную систему (от этого будет зависеть
сложность алгоритмов: sync --- достаточно простые алгоритмы). Но достаточно
сложные алгоритмы существуют (к примеру, BitCoin).


\section{Алгоритмы на распределенных системах}

Алгоритмы на распределенных системах решают злободневные задачи.
\begin{example*}
Разделяемый регистр (Shared Register). В ней есть две операции: 
\begin{lyxcode}
->~write(v)

->~read()~->~v
\end{lyxcode}
\end{example*}
Но система настолько сложна (проблема репликации, проблема разрыва
и тому подобное). При переходе на ненадежные системы простейшие операции
могут быть достаточно сложны


\begin{example*}
Broadcasting --- надо гарантировать, что до каждого узла дошло определенное
значение. Если система надежная, то можно послать все подряд. Но могут
быть проблемы: 1) узел ушел, а затем вернулся; 2) узел, посылающий
сообщения, сдох. Небольшое решение проблемы: получил одно, передай
другим, и пометь сам, что получил сообщение.
\end{example*}

\begin{example*}
Consensus --- есть операции $propose\rightarrow x$, $decide\leftarrow x$.
Несколько узлов посылают значения, а затем решается, какое значение
выбрать.
\end{example*}

\begin{example*}
Leader Election --- иногда в равноправной системе надо выбрать лидера
самостоятельно. При этом мы хотим, чтобы лидер всегда был один. Могут
происходить события $leader\leftarrow n$.
\end{example*}
Проблемы с алгоритмами начинаются в том случае, когда от системы требуется
меньше проблем.

Далее будем рассматривать модель \textbf{Consensus }в случае Fair-Recovery.
\begin{lyxalgorithm*}
(PAXOS) В данной системе будут существовать определенные роли:\end{lyxalgorithm*}
\begin{enumerate}
\item Client.
\item Voter (Acceptor).
\item Coordinator (Proposer).
\item Learner.
\end{enumerate}
Изначально клиент выбирает proposer и посылает ему propose на достижение
консенсунса. Далее proposer выбирает номер попытки и посылает сообщение
$prepare(i)$. Часть из них будет отвечать на этот запрос: посылается
$ack(i,x)$ или опровержение $nak(j)$. У каждого $voter$ есть текущее
значение $i$ и $x$. Если $propose$ получает больше половины подтверждений.
Если все посылки были пустыми, то посылается далее $accept$ на $voter$.
Если в этом случае будет $accept$, то $voter$ посылает сообщение
на $learner$. И если $learner$ получает более половины подтверждений
$decide$, то алгоритм считается выполненным. Иначе если получаем
$nak(j),$ то можно послать $nak(j+1)$. Больше половины --- чтобы
кворумы пересекались, и система могла восстановить актуальную информацию.

Если оторвался $voter,$ то ничего страшного. Но есть проблема, если
отлетел $proposer$, то тогда клиент может начать посылать узлы $p_{2}$.
Но при этом такая система может реагировать нормально (в итоге, до
$learner$ дойдет два сообщения $decide\ x$, хотя было на $p_{2}$
$decide\ y$). Пример:
\begin{enumerate}
\item $P1\rightarrow decide\ x$
\item $V\rightarrow ack(1,empty)$
\item $P2\rightarrow prepare\ 2$
\item $P2\rightarrow ack(2,empty)$
\item $Wake\ 1$
\item $decide\ y\ from\ p_{2}$
\item $P1\rightarrow accept(1,x)$
\item $V\rightarrow nak(2)$
\item $P1\rightarrow prepare(3)$
\item $V\rightarrow ack(3,y)$
\item $P1\rightarrow accept(3,y)$
\item $ack$
\item $decide\ y$
\end{enumerate}
В итоге, можно заметить, что системе неважно, к какому значению пришел
результат.

Самая плохая ситуация:
\begin{enumerate}
\item $P1\rightarrow decide\ x$
\item $V\rightarrow ack(1,empty)$
\item $P2\rightarrow prepare\ 2$
\item $P2\rightarrow ack(2,empty)$
\item $Wake\ 1$
\item $P1\rightarrow accept(1,x)$
\item $nak(2)\rightarrow P1$
\item $P1\rightarrow prepare(3)$
\item $Ack(3,empty)\rightarrow P1$
\item $P2\rightarrow accept(2,y)$
\item $Nak(3)\rightarrow P2$
\item $P2\rightarrow prepare(4)$
\item $Ack(4,empty)\rightarrow P2$.
\end{enumerate}
И такой процесс может продолжаться игра. Но нельзя гарантировать одновременно
$leaveness$ и $safety$. Решение проблем: 1) крут тот, кто имеет
высокий приоритет; 2) выполнить $sleep$ на определенное время. Тогда
вероятность завершения процесса очень сильно увеличивается. Таким
образом, алгоритм сойдется почти наверное.

В реальной жизни, внутри одного узла может выполняться сразу несколько
ролей. Этот алгоритм адресован к одному консенсусу (после этого произойдет
крах, и желательно закопатить узлы). Поэтому новая попытка достичь
консенсус не должна зависеть от предыдущей. Обычно эта проблема разрешается
установкой определенного $id$ к номеру достижения консенсуса. Данный
алгоритм является основным блоком для достижения согласия.


\section{Распределенные базы данных}

База данных --- набор структурированных данных, удобных для модификации
и использованиии и интерпретируемых через некоторый понятный человеку
язык. Стандартная модель --- магазин: $customer$, $order$, $goods$.
В базе данных есть система управления БД (СУБД), которая позволяет
выполнять операции $SELECT$, $UPDATE$, $INSERT$, $DELETE$. Плюс
еще есть концепция транзакций: атомарной в предметной области операции
(все выполнены или ни одно не выполнено).

Свойства транзакций:
\begin{enumerate}
\item Атомарность
\item Консистентность ($consinstency$) --- после транзакций сохраняются
инвариантов.
\item Изолированность ($isolation$) --- параллельные транзакции выполняются
последовательно.
\item Долгожительность ($durability$) --- после коммита откат самостоятельно
выполнен быть не может.
\end{enumerate}
Появляется такое сочетание, как $ACID$. Но в реальных БД это не так:
обычно, бывают проблемы с изолированностью.
\begin{example}
Есть два узла, каждое из которых хранит одно число (как общее хранилище),
и мы хотим делать операции $read$ и $write$. При $write$ мы бы
хотели записывать в оба узла. То есть, если есть значения $x_{1}$
и $x_{2}$ с инвариантом $x_{1}=x_{2}$. И создаются многочисленные
проблемы, аналогичные рассмотренным в прошлом разделе.
\end{example}
В обычной БД либо она работает, либо она падает. В распределенной
БД некоторые операции могут дойти и выполниться, а некоторые --- нет.
Поэтому к распределенным БД были выдвинуты следующие требования:
\begin{enumerate}
\item \textbf{Availability} (доступность) --- в работоспособной системе
любая операция должна успешно завершаться за конечное время.
\item \textbf{Consistency }(согласованность/линеаризуемость) --- в идеальном
мире запрос получается запрос, и сразу на него получается ответ, но
появляется прослойка клиента. Поэтому появляется некоторый промежуток
времени для отправки сообщения и получения данных. Если операции не
пересекаются, то все OK. Линеаризуемость: если операции пересекаются
по времени, то они выполняются, но при этом далее будет гарантированно
получено, а операция, которая закончилась ранее, не видит результат
операции (к примеру, $write$).
\item \textbf{Partition tolerance }--- сеть между узлами может меняться
на сколь угодно большое время, и она должна как-то жить с этим (не
предъявляет надежность). При этом понятно, что мы не должны ждать
операции с availability (гарантии выполнены).
\end{enumerate}
В 2000 году была сформулирована теорема.
\begin{thm*}
(CAP-теорема) Все три свойства не могут быть одновременно выполнены
в распределенной системе\end{thm*}
\begin{proof}
Идея доказательства: если требовать надеждную сеть, то получим $!P$,
если отказ на запись --- то $!A$, если необходим $stale$ $read$,
то не $!C$
\end{proof}
Таким образом, можно соорудить такой треугольник:
\begin{enumerate}
\item \textbf{CP-systems }--- пытаемся раскинуть все данные по узлам, и
запускаем алгоритм достижения консенсуса, поэтому есть кто-то отлетел,
то ребята собирают кворум и совместно договариваются. Другой подход
--- проксирование через лидера (выбор лидера, а затем незаметно пересылают
данные лидеру, который их упорядочивают). Если лидер умирает, то выбирается
новый лидер. Так как можно 
\item \textbf{AP-systems }--- храним всю информацию в копиях, а затем на
запись говорим, что запрос выполнен, а откладываем ее на потом (появляется
понятие \textit{eventual consistency} --- существует момент, когда
появится необходимая запись). $ $
\item \textbf{AC-systems }--- если нет линка, то говорим, что ненадежное
состояние.
\end{enumerate}
CAP-теорема дала новый толчок. Но бывают БД, которые сложно причислить
к одному из типов треугольника (они почти есть, но строгой математической
основы нет). Поэтому внутри треугольника может быть множество подтипов.
В реальности, мы не можем отказаться от partition tolerance, но на
самом деле, между $A$ и $\text{С}$ нет дихотомии. А если лучше задуматься,
то мы можем создать барицентрическую систему координат (тетраэдр),
в которой можно получить лучшее ускорение.

Насчет $A$, можно договориться, какие операции выполнять, какие нет.
Насчет $C$, необходимо придумать правила согласованности. И таких
моделей больше одной:
\begin{enumerate}
\item \textbf{Linearization }--- есть дополнительный аспект: если между
write есть read, и получены новые данные, то мы должны во второй раз
вернуть новое значение
\item \textbf{Sequential consistency }--- концепция последовательного доступа
с одного клиента. Пример доставки: если есть операция от клиента,
то он говорит ОК и доставляет выполняет после.\textbf{ }Далее на другом
узле происходит синхронизация, поэтому новый откат может привести
к чтению назад. Аналогичная ситуация происходит и в Твиттере. Есть
проблемы: 1) write buffering, 2) read caching.
\item \textbf{Casual consistency }--- некоторые операции должны соблюдаться
причинно-следственные связи (такие операции должны быть выполнены
последовательно). Остальные операции могут быть выполнены параллельно
(к примеру, комментарии в LiveJournal могут появляться параллельно).
\item \textbf{Serializability }--- никаких требований по упорядочиванию
операции. С одной стороны эта модель слабая, а с другой --- очень
сильная: хотим глобальной картины мира, при этом операция либо сразу
подтверждается. В итоге, сериализуемость не всегда может быть сериализована.
Таким образом, сериализуемость дает порядок на глобальной картине
мира.
\end{enumerate}
Конечно, есть еще более слабые связи.
\begin{example*}
Если три узла: $n_{1}$, $n_{2},$ $n_{3}$. $\text{\ensuremath{client\rightarrow n_{2}\rightarrow n_{2}\rightarrow return}}$.
Далее, $client\rightarrow n1\rightarrow n3\rightarrow fail$. А в
конце, $client_{2}\rightarrow n_{2}\rightarrow10$. Произошло получение
записи, которой нет. (read ancommited). Небольшой плюс --- производительность.
\end{example*}
Чем больше мы двигаемся в линеаризуемость, тем больше мы должны платить.
Иногда важно только то, что мы получили некоторую информацию.

Если у нас есть логика: после $fail$ отдать новый $write$, то получаем
$eventual$ $consistency$.

В итоге, когда проектируем распределенную систему, надо выбрать уровень
согласованности, который нам необходимо.
\begin{example*}
Если банкомат не подключен к сети, то необходимо сделать выбор: банк
идет на поступок --- идет упор на доступность, а если баланс ушел
в нуль, то мы потом будем начислять проценты (из-за своей несогласованности).
\end{example*}
Реальные базы данных редко находятся в одной точки треугольника (обычно
идет тонкая настройка относительно необходимой операции).
\begin{example*}
Есть Postgres (конец 1990-ых годов), master и набор slave-ов. В Master
node можно отсылать операции (SELECT, INSERT, UPDATE, DELETE), а тот
реплицирует на slave-ы (master подтверждает их). А запросы можно отправлять
на slave-ы или на Master. Но надо необходимо назначать master и slave-ы.
Поэтому если происходят проблемы, то происходит read-only чтение до
тех пор, пока не придет человек и починит. Но сейчас уже есть сериализуемость
и линейность. Но и этот подход не очень: кластер отправляется в read-only,
или сделали запрос, в этот момент ломается сеть (частично происходит
запись, происходит fail, но при этом могут появляться ошибочные записи
об ошибками). Поэтому надо стараться делать записи индепотентными
(повторять их). Еще одно решение - создать primary key на каждом узле
отдельно. update set x = 10 иденпотентная, а update set x = x + 1
--- нет. Postgres дает реляционный подход к данным, и по идее, он
и применяется (в области распределенных реляционных базах данных вроде
бы ничего нет).
\end{example*}

\begin{example*}
База данных \textbf{Redis}. Напомним, что в треугольнике между C и
A можно свободно двигаться в терминах CAP-теоремы. Redis в этом месте
сдвинут в сторону согласованности и заточен под высокую производительность
операций. У нас есть набор узлов (node), зачастую работает чисто с
памятью. Redis --- простой key-value хранилище, есть операции $read(k)\rightarrow v$,
$write(k,v)$, $cas(k,old,new)$ --- запись новых данных, если сравнение
со старым значением проходит. Есть выделенный мастер, который обрабатывает
все запросы на изменение. Redis настроен на производительность, делает
операции локально, подтверждает их, а затем асинхронно реплицирует
их (не смотря на результат). Достоинством такого подхода является
следующее: интервал между $write$ очень мал, около 10000 операций
в секунду. Если master умирает, то система приходит в хаос. Перевыбор
мастера происходит так: если мастер находится не в большинстве, то
происходит перевыбор. Пример: 5 узлов, разбиение на 2-3 (мастер там,
где два узла). Поэтому есть два мастера, и кластер уходит в полную
рассинхронизацию. В итоге, после восстановления, большая часть реплик
потеряется. Процент потерь порядка $56\%$. С доступностью у нас проблемы,
с согласованностью тоже. Но есть производительность. Поэтому их можно
держать в кешах, комментарии пользователей на предобработке (данные
не предоставляют ценности, ибо лежат еще где-то или нужны на короткое
время). 
\end{example*}

\begin{example*}
База данных \textbf{MongoDB}. Эта база данных --- не SQL, но дает
больше простора. Она хранит документы в формате JSON.\end{example*}
\begin{lyxcode}
\{''name'':~``john'',~``orders'':~{[}1,2,3{]},~``foo'':~\{''x'':~``y'',~...\}\}
\end{lyxcode}
Похоже по возможностям SQL, но можно делать изменения в рамках одной
таблице. Репликация происходит аналогично Redis: есть кластер и выделенный
мастер. Мастер выбирается репликами сам (как и в Redis). Только мастер
обрабатывает записи, и только он имеет read-only. Но если нам необходим
stale-read (еще не произошла реплика на новые данные). Все операции
read происходят из одной реплики (какая к нам ближе, к той можем и
обращаться). Но на write есть методология write concern: список того,
что надо сделать, чтобы запись стала успешной.
\begin{enumerate}
\item 0 --- все разрешено.
\item 1 --- мастер работает аналогично Redis.
\item 2 --- запись должна быть обработана на мастере и на одной из реплик
(первая успешная репликация не на мастере возвращает OK). Происходит
защита данных при отрубании одного из узлов (но проблемы связности
есть).$Majority$ $(\frac{n}{2}+1)$ --- перевыборы мастера проходят
спокойно: выберется машина с более актуальными данными. Все операции
в Mongo журналируемы, поэтому они договариваются. Но нет транзакционности.
К примеру:

\begin{enumerate}
\item $read\rightarrow5$
\item $write\ 10\rightarrow fail$ (операция происходила локально, но на
репликации ничего не произошло, и новые данные будут возвращены)
\item $read\rightarrow10$
\end{enumerate}

Таким образом, нет атомарных коммитов на данные. 

\end{enumerate}
В момент разбиения могут быть stale reads, помимо dirty reads: перевыборы
происходят не одновременно (два узла считают себя мастерами). К примеру,
идет запись $w\ 10\rightarrow M2$, проходит OK, а на $M1$ может
пройти read через 5. Но если мы готовы мириться с небольшими простоями,
более того, можно использовать $compare-swap$, есть в виде $findAndModify$,
который еще и возращает документ. Поэтому если отправить на majority,
то все договариваются, появляется линеаризуемость, но производительность
в этом случае сильно падает.

Далее будем говорить о базах с $high$ $availability$ (предыдущие
базы старались удовлетворять согласованность насколько можно). 
\begin{example*}
База данных Riak (последователь DynamoDB). Это key-value storage (strings).
Есть достаточно большое число узлов, мы считаем несколько хешей от
ключа (обычно считают один хеш, а дальше инкремент). Все хеши отображаются
на общий круг, который делится на секторы приблизительно равного размера
(консистентое хеширование). Несколько виртуальных узлов объединены
одним физическим узлов. Когда мы хотим выполнить операцию, то считаем
хеш, идем в ту область, где он может храниться. При этом стараются
сделать так, чтобы соседние данные лежат на разных нодах. При этом
каждый узел (в нашем случае, полагаем три) можно выполнять read-write
операции. При этом если через $R$ обозначить число узлов на чтение,
а $W$ --- на запись, то неравенство $R+W\ge N$ дает согласованность
системы. При этом мы читаем данные последней актуальности. Если в
нашем случае $R=W=1$, то имеем полную свободу на каждом узле, и при
разделении надо будет сливать данные. Поэтому, варьируя значения величин
$R$ и $W$, можно настраивать систему. К примеру, если $R=1$, $W=N$,
то имеем полное чтение, но запись очень медленная и требует согласованности
всех данных, поэтому они согласованны. Если $R=N$, $W=1$, то получаем
обратную ситуацию (только сливать данные непонятно как). Что произойдет
в случае одновременной записи на разных репликах? Первый подход ---
кто последний записал, тот и молодец. Но такой подход некорректен,
ибо нельзя достичь консенсунса. К примеру, если $R=2$, $W=2$. Если
мы питаемся работать на $\text{{\ensuremath{M_{1}},\ensuremath{M_{2}}}}$
и $\{M_{3}\}$, то тогда первый записал, репллицировал, но при этом
с $M_{3}$ можно пробить данные на первый узел (затерев старые данные).
Второй подход ($CRDT$) --- при одновременной операции разрешение
конфликтов отправляется пользователю. Но если операция ассоциативная,
коммутативная, идемпотентая, то можно применить композицию ($(x_{0}\varoplus a)$
$(x_{0}\varoplus b)\rightarrow(x_{0}\varoplus a\varoplus b)$), и
Riak может предоставлять возможность для слияния данных. При $R=W=1$,
можно достигнуть $eventual\ consistency$, но даже $read\ your\ rights$
(нельзя даже гарантировать, что записанные свои данные не будут получены).
\end{example*}

\begin{example*}
База данных Cassandra. Заточена под запись чисто новых данных в достаточно
больших количествах (обновлять данные \textbf{нельзя}). Поэтому нет
проблемы разрешения конфликтов (access log отлично подходит для этого,
и поэтому про нее можно забыть). Раз писать надо много, то у нас $W=1$,
при этом $R=N$ (обычно аггрегирующий запрос для некоторой статистики).
В остальном Riak и Cassandra похожи. Внутреннее устройство: есть журналы
на каждом узле (SST --- sorted table, данные скидываются в сортированную
таблицу), а далее создаем новый журнал. Чтобы найти сортированные
данные, можно использовать бинпоиск, а слияние --- $MergeSort$. Чтобы
$SST$ не плодились, происходит чистка, и создается новая, более новая
таблица (происходит подмена метаданных). Между узлами выглядит аналогично
Riak.\end{example*}
\begin{cor*}
Есть два подхода в строительстве распределенных систем: 1) есть мастер
--- получаем большую согласованность; 2) (R/W/N) есть запросы на все
узлы, но при этом нет консистентности, можно достичь eventual consistency.
Но линеаризуемость достигается только некоторыми ухищрениями (но это
практически не нужно). Если нам нужны все изменения, то нужен только
write-only, иначе --- мириться с этим.\end{cor*}
\begin{example*}
База данных ZooKeeper. В каждой записи около сотен данных, хранит
всякого рода конфигурации систем. Есть несколько узлов, и на каждую
операцию запускается поиск консенсуса. Это одна из немногих баз, которая
обеспечивает строгую линеаризацию в условии ненадежных сетей.\end{example*}

\end{document}
